import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression 
from sklearn.preprocessing import PolynomialFeatures 
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso
from scipy import stats


df1  = pd.read_csv("/Users/ethanboroditsky/ML_Focus/FocusFile1.csv")
df1 = swap_columns(df1, "y", "x7")
#print(df1.shape)
df2 = df1.dropna()
#print(df2.shape)
df2 = df2[(np.abs(stats.zscore(df)) < 3).all(axis=1)]


# create set of variables to pass to PCA, x's only / exclude Y
vars = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7']
x = df2.loc[:, vars].values
# also create Y while we're at it for use later on in regressions
y = df2.loc[:, 'y'].values
# normalize x
x_norm = StandardScaler().fit_transform(x)


#I only used x2 because no other x variable predicted y in a meaningful way; a uniform
#distribution x vs y for x not equal x2


IVs = ['x2']
# create train / test split using dataframe
x_train, x_test, y_train, y_test = train_test_split(df2.loc[:, IVs], df2.loc[:, 'y'], test_size=0.25, random_state=13)


print (x_train.shape, y_train.shape)
print (x_test.shape, y_test.shape)


linear_model = LinearRegression(normalize=True)
p2_model = LinearRegression(normalize=True)
p3_model = LinearRegression(normalize=True)


p2_features = PolynomialFeatures(degree=2)
p2_train = p2_features.fit_transform(x_train)
p2_test = p2_features.fit_transform(x_test)


p3_features = PolynomialFeatures(degree=3)
p3_train = p3_features.fit_transform(x_train)
p3_test = p3_features.fit_transform(x_test)


# now do estimation of models
lin_1 = linear_model.fit(x_train, y_train)
p2_1 = p2_model.fit(p2_train, y_train)
p3_1 = p3_model.fit(p3_train, y_train)


# predict values for test sets
lin1_predict = lin_1.predict(x_test)
p2_predict = p2_1.predict(p2_test)
p3_predict = p3_1.predict(p3_test)


print(mean_squared_error(lin1_predict, y_test))
print(mean_squared_error(p2_predict, y_test))
print(mean_squared_error(p3_predict, y_test))


#returned:→
#6674788.886035033
#17896.562073278914
#17815.936909275064


p2_coefficients = p2_1.coef_
p2_intercept = p2_1.intercept_


print("\nPolynomial Regression (Degree 2) Equation:")
terms = " + ".join(f"{p2_coefficients[i]:.4f}*x^{i}" for i in range(len(p2_coefficients)))
print(f"y = {p2_intercept:.4f} + {terms}")


#returned → 
Polynomial Regression (Degree 2) Equation:
y = 680.4569 + 1.4211*x2^1 + -2.9813*x2^2